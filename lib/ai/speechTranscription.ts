import OpenAI from 'openai';
import { chatModel, isAIEnabled } from './config';
import { PromptTemplate } from '@langchain/core/prompts';
import { StringOutputParser } from '@langchain/core/output_parsers';

// Configuration OpenAI pour Whisper
const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

interface TranscriptionResult {
  originalText: string;
  improvedText: string;
  confidence: number;
  language: string;
  fallbackMode?: boolean;
}

// Template pour am√©liorer les transcriptions
const transcriptionImprovementPrompt = PromptTemplate.fromTemplate(`
Tu es un expert en am√©lioration de transcriptions vocales pour des signalements d'urgence √† Abidjan, C√¥te d'Ivoire.

TRANSCRIPTION ORIGINALE :
{originalText}

CONTEXTE :
- Signalement d'urgence ou de s√©curit√©
- Peut contenir des noms de lieux d'Abidjan (Cocody, Plateau, Adjam√©, Marcory, Koumassi, Treichville)
- Peut contenir des termes en fran√ßais ivoirien
- Peut avoir des erreurs de transcription automatique

INSTRUCTIONS :
1. Corrige les erreurs de transcription √©vidents
2. Am√©liore la clart√© et la structure
3. Pr√©serve le sens original
4. Adapte le vocabulaire au contexte d'urgence
5. Corrige les noms de lieux d'Abidjan si mal transcrits
6. Garde un style direct et informatif
7. Maximum 200 mots

EXEMPLES DE CORRECTIONS :
- "je vois de la fum√©e" ‚Üí "Je vois de la fum√©e"
- "kokodie" ‚Üí "Cocody"  
- "il y a un probl√®me" ‚Üí "Il y a un accident/incident"

R√©ponds UNIQUEMENT avec le texte am√©lior√©, sans guillemets ni formatage.
`);

export class SpeechTranscriptionService {
  private improvementChain;

  constructor() {
    const outputParser = new StringOutputParser();
    this.improvementChain = transcriptionImprovementPrompt.pipe(chatModel).pipe(outputParser);
  }

  // Transcrire un fichier audio avec Whisper
  async transcribeAudio(audioBlob: Blob): Promise<TranscriptionResult | null> {
    if (!isAIEnabled()) {
      console.log('üé§ IA d√©sactiv√©e - transcription indisponible');
      return null;
    }

    try {
      console.log('üé§ Transcription audio avec Whisper...');

      // Cr√©er un File √† partir du Blob
      const audioFile = new File([audioBlob], 'audio.webm', { type: 'audio/webm' });

      // Transcrire avec Whisper
      const transcription = await openai.audio.transcriptions.create({
        file: audioFile,
        model: 'whisper-1',
        language: 'fr', // Fran√ßais
        prompt: 'Signalement d\'urgence √† Abidjan, C√¥te d\'Ivoire. Quartiers: Cocody, Plateau, Adjam√©, Marcory, Koumassi, Treichville.',
      });

      const originalText = transcription.text;
      console.log('‚úÖ Transcription Whisper termin√©e:', originalText);

      // Am√©liorer la transcription avec GPT-4o-mini
      let improvedText = originalText;
      try {
        console.log('ü§ñ Am√©lioration de la transcription avec GPT-4o-mini...');
        improvedText = await this.improvementChain.invoke({
          originalText: originalText
        });
        console.log('‚úÖ Am√©lioration termin√©e:', improvedText);
      } catch (error) {
        console.warn('‚ö†Ô∏è Erreur am√©lioration transcription, utilisation du texte original:', error);
        improvedText = originalText;
      }

      return {
        originalText,
        improvedText: improvedText.trim(),
        confidence: 90, // Whisper a g√©n√©ralement une bonne pr√©cision
        language: 'fr'
      };

    } catch (error) {
      console.error('‚ùå Erreur lors de la transcription:', error);
      return null;
    }
  }

  // Fallback simple pour quand l'API n'est pas disponible
  generateFallbackTranscription(): TranscriptionResult {
    return {
      originalText: 'Transcription vocale indisponible',
      improvedText: 'La transcription vocale n\'est pas disponible actuellement. Veuillez saisir votre signalement manuellement.',
      confidence: 0,
      language: 'fr',
      fallbackMode: true
    };
  }

  // Valider et nettoyer une transcription
  validateTranscription(text: string): { isValid: boolean; cleanedText: string } {
    const cleaned = text.trim();
    
    // V√©rifications basiques
    const isValid = cleaned.length > 5 && 
                   cleaned.length < 1000 && 
                   !cleaned.toLowerCase().includes('transcription failed');

    return {
      isValid,
      cleanedText: cleaned
    };
  }

  // D√©tecter la langue de la transcription (basique)
  detectLanguage(text: string): string {
    const frenchWords = ['le', 'la', 'les', 'un', 'une', 'est', 'sont', 'dans', 'sur', 'avec', 'pour'];
    const englishWords = ['the', 'and', 'is', 'are', 'in', 'on', 'with', 'for'];
    
    const words = text.toLowerCase().split(' ');
    
    const frenchCount = words.filter(word => frenchWords.includes(word)).length;
    const englishCount = words.filter(word => englishWords.includes(word)).length;
    
    return frenchCount > englishCount ? 'fr' : 'en';
  }

  // Formatter la transcription pour un signalement
  formatForReport(transcription: TranscriptionResult): { title: string; description: string } {
    const text = transcription.improvedText;
    
    // Essayer d'extraire un titre (premier bout de phrase)
    const sentences = text.split(/[.!?]+/);
    let title = sentences[0]?.trim() || 'Signalement vocal';
    
    // Limiter le titre √† 50 caract√®res
    if (title.length > 50) {
      title = title.substring(0, 47) + '...';
    }
    
    // La description est le texte complet
    let description = text;
    
    // Ajouter des informations contextuelles si la transcription est courte
    if (description.length < 20) {
      description += ' (Signalement transmis par message vocal)';
    }
    
    return {
      title: title || 'Signalement vocal',
      description
    };
  }
}

export const speechTranscription = new SpeechTranscriptionService(); 